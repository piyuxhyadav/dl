{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8148442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca8f1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Deep learning also known as deep structured learning\", \n",
    "\"is part of a broader family of machine learning methods based\", \n",
    "\"on artificial neural networks with representation learning\", \n",
    "\"Learning can be supervised, semi-supervised or unsupervised\",\n",
    "\"Deep-learning architectures such as deep neural networks\", \n",
    "\"deep belief networks, deep reinforcement learning\", \n",
    "\"recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision  speech recognition, natural language processing, machine translation\", \n",
    "\"where they have produced results comparable to and in some cases surpassing human expert performance\"\n",
    "]\n",
    "# dl_data = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "863b1656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 75\n",
      "Vocabulary Sample: [('learning', 1), ('deep', 2), ('networks', 3), ('neural', 4), ('and', 5), ('as', 6), ('of', 7), ('machine', 8), ('supervised', 9), ('have', 10)]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(dl_data)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "# build vocabulary of unique words\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in data]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2 # context window size\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c510aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2182df95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['deep', 'learning', 'known', 'as'] -> Target (Y): also\n",
      "Context (X): ['learning', 'also', 'as', 'deep'] -> Target (Y): known\n",
      "Context (X): ['also', 'known', 'deep', 'structured'] -> Target (Y): as\n",
      "Context (X): ['known', 'as', 'structured', 'learning'] -> Target (Y): deep\n",
      "Context (X): ['is', 'part', 'a', 'broader'] -> Target (Y): of\n",
      "Context (X): ['part', 'of', 'broader', 'family'] -> Target (Y): a\n",
      "Context (X): ['of', 'a', 'family', 'of'] -> Target (Y): broader\n",
      "Context (X): ['a', 'broader', 'of', 'machine'] -> Target (Y): family\n",
      "Context (X): ['broader', 'family', 'machine', 'learning'] -> Target (Y): of\n",
      "Context (X): ['family', 'of', 'learning', 'methods'] -> Target (Y): machine\n",
      "Context (X): ['of', 'machine', 'methods', 'based'] -> Target (Y): learning\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "#     print(x, y)\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b4dbc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 4, 100)            7500      \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 75)                7575      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,075\n",
      "Trainable params: 15,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "# build CBOW architecture\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# view model summary\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af7ee8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 370.9859700202942\n",
      "\n",
      "Epoch: 2 \tLoss: 365.95088243484497\n",
      "\n",
      "Epoch: 3 \tLoss: 360.81728649139404\n",
      "\n",
      "Epoch: 4 \tLoss: 354.84530425071716\n",
      "\n",
      "Epoch: 5 \tLoss: 347.90175580978394\n",
      "\n",
      "Epoch: 6 \tLoss: 340.07426285743713\n",
      "\n",
      "Epoch: 7 \tLoss: 331.6740012168884\n",
      "\n",
      "Epoch: 8 \tLoss: 323.16635966300964\n",
      "\n",
      "Epoch: 9 \tLoss: 315.0267950296402\n",
      "\n",
      "Epoch: 10 \tLoss: 307.56587851047516\n",
      "\n",
      "Epoch: 11 \tLoss: 300.8361974954605\n",
      "\n",
      "Epoch: 12 \tLoss: 294.68997848033905\n",
      "\n",
      "Epoch: 13 \tLoss: 288.92508405447006\n",
      "\n",
      "Epoch: 14 \tLoss: 283.3787097334862\n",
      "\n",
      "Epoch: 15 \tLoss: 277.96111285686493\n",
      "\n",
      "Epoch: 16 \tLoss: 272.6375021934509\n",
      "\n",
      "Epoch: 17 \tLoss: 267.39986634254456\n",
      "\n",
      "Epoch: 18 \tLoss: 262.2533531188965\n",
      "\n",
      "Epoch: 19 \tLoss: 257.2086030244827\n",
      "\n",
      "Epoch: 20 \tLoss: 252.2792191505432\n",
      "\n",
      "Epoch: 21 \tLoss: 247.4753773212433\n",
      "\n",
      "Epoch: 22 \tLoss: 242.80450057983398\n",
      "\n",
      "Epoch: 23 \tLoss: 238.27022087574005\n",
      "\n",
      "Epoch: 24 \tLoss: 233.8772838115692\n",
      "\n",
      "Epoch: 25 \tLoss: 229.62362802028656\n",
      "\n",
      "Epoch: 26 \tLoss: 225.50446462631226\n",
      "\n",
      "Epoch: 27 \tLoss: 221.51664543151855\n",
      "\n",
      "Epoch: 28 \tLoss: 217.65497481822968\n",
      "\n",
      "Epoch: 29 \tLoss: 213.91231113672256\n",
      "\n",
      "Epoch: 30 \tLoss: 210.28350540995598\n",
      "\n",
      "Epoch: 31 \tLoss: 206.76119643449783\n",
      "\n",
      "Epoch: 32 \tLoss: 203.33681014180183\n",
      "\n",
      "Epoch: 33 \tLoss: 200.00287374854088\n",
      "\n",
      "Epoch: 34 \tLoss: 196.75316597521305\n",
      "\n",
      "Epoch: 35 \tLoss: 193.58014398813248\n",
      "\n",
      "Epoch: 36 \tLoss: 190.4776972234249\n",
      "\n",
      "Epoch: 37 \tLoss: 187.43773558735847\n",
      "\n",
      "Epoch: 38 \tLoss: 184.45302924513817\n",
      "\n",
      "Epoch: 39 \tLoss: 181.5191870033741\n",
      "\n",
      "Epoch: 40 \tLoss: 178.63146367669106\n",
      "\n",
      "Epoch: 41 \tLoss: 175.78762778639793\n",
      "\n",
      "Epoch: 42 \tLoss: 172.98657951503992\n",
      "\n",
      "Epoch: 43 \tLoss: 170.22698632627726\n",
      "\n",
      "Epoch: 44 \tLoss: 167.50752557069063\n",
      "\n",
      "Epoch: 45 \tLoss: 164.82308612763882\n",
      "\n",
      "Epoch: 46 \tLoss: 162.1712704822421\n",
      "\n",
      "Epoch: 47 \tLoss: 159.5499738305807\n",
      "\n",
      "Epoch: 48 \tLoss: 156.95809992402792\n",
      "\n",
      "Epoch: 49 \tLoss: 154.39631429314613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 50):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d061415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>-0.361941</td>\n",
       "      <td>0.731627</td>\n",
       "      <td>0.239838</td>\n",
       "      <td>-0.531324</td>\n",
       "      <td>-0.047572</td>\n",
       "      <td>0.423992</td>\n",
       "      <td>0.029935</td>\n",
       "      <td>-0.117340</td>\n",
       "      <td>0.682427</td>\n",
       "      <td>-0.012576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154055</td>\n",
       "      <td>-0.311902</td>\n",
       "      <td>0.215040</td>\n",
       "      <td>-0.053152</td>\n",
       "      <td>-0.465848</td>\n",
       "      <td>-0.186827</td>\n",
       "      <td>-0.131936</td>\n",
       "      <td>0.609785</td>\n",
       "      <td>-0.010017</td>\n",
       "      <td>0.745071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>networks</th>\n",
       "      <td>0.491174</td>\n",
       "      <td>-0.180589</td>\n",
       "      <td>0.282849</td>\n",
       "      <td>-0.213955</td>\n",
       "      <td>-0.121375</td>\n",
       "      <td>0.487923</td>\n",
       "      <td>0.510078</td>\n",
       "      <td>-0.157923</td>\n",
       "      <td>0.163060</td>\n",
       "      <td>0.383088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.484411</td>\n",
       "      <td>-0.257293</td>\n",
       "      <td>0.248224</td>\n",
       "      <td>0.164831</td>\n",
       "      <td>0.715166</td>\n",
       "      <td>-0.475854</td>\n",
       "      <td>-0.674728</td>\n",
       "      <td>-0.776752</td>\n",
       "      <td>-0.227652</td>\n",
       "      <td>-0.530007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural</th>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>-0.496395</td>\n",
       "      <td>-0.302921</td>\n",
       "      <td>0.086413</td>\n",
       "      <td>0.379350</td>\n",
       "      <td>0.361062</td>\n",
       "      <td>0.326057</td>\n",
       "      <td>-0.039550</td>\n",
       "      <td>-0.077444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806426</td>\n",
       "      <td>0.144321</td>\n",
       "      <td>0.477996</td>\n",
       "      <td>-0.104747</td>\n",
       "      <td>-0.834805</td>\n",
       "      <td>0.799360</td>\n",
       "      <td>-0.066161</td>\n",
       "      <td>0.337029</td>\n",
       "      <td>-0.684721</td>\n",
       "      <td>0.493491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.169238</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>-0.065764</td>\n",
       "      <td>-0.162544</td>\n",
       "      <td>-0.519765</td>\n",
       "      <td>-0.532053</td>\n",
       "      <td>0.369184</td>\n",
       "      <td>0.325808</td>\n",
       "      <td>0.105479</td>\n",
       "      <td>0.568380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.207473</td>\n",
       "      <td>0.022182</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>-0.383148</td>\n",
       "      <td>-0.540944</td>\n",
       "      <td>-0.025271</td>\n",
       "      <td>0.476111</td>\n",
       "      <td>-0.004251</td>\n",
       "      <td>-0.621524</td>\n",
       "      <td>-0.243947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>0.068114</td>\n",
       "      <td>0.071128</td>\n",
       "      <td>0.080918</td>\n",
       "      <td>-0.198025</td>\n",
       "      <td>-0.190036</td>\n",
       "      <td>-0.055908</td>\n",
       "      <td>-0.338354</td>\n",
       "      <td>-0.281027</td>\n",
       "      <td>-0.422187</td>\n",
       "      <td>-0.105510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.264047</td>\n",
       "      <td>-0.070758</td>\n",
       "      <td>0.375570</td>\n",
       "      <td>0.088351</td>\n",
       "      <td>-0.044989</td>\n",
       "      <td>0.018857</td>\n",
       "      <td>0.156525</td>\n",
       "      <td>-0.378772</td>\n",
       "      <td>0.033704</td>\n",
       "      <td>0.002676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5    \n",
       "deep     -0.361941  0.731627  0.239838 -0.531324 -0.047572  0.423992  \\\n",
       "networks  0.491174 -0.180589  0.282849 -0.213955 -0.121375  0.487923   \n",
       "neural    0.141732  0.227724 -0.496395 -0.302921  0.086413  0.379350   \n",
       "and       0.169238  0.034286 -0.065764 -0.162544 -0.519765 -0.532053   \n",
       "as        0.068114  0.071128  0.080918 -0.198025 -0.190036 -0.055908   \n",
       "\n",
       "                6         7         8         9   ...        90        91   \n",
       "deep      0.029935 -0.117340  0.682427 -0.012576  ... -0.154055 -0.311902  \\\n",
       "networks  0.510078 -0.157923  0.163060  0.383088  ... -0.484411 -0.257293   \n",
       "neural    0.361062  0.326057 -0.039550 -0.077444  ...  0.806426  0.144321   \n",
       "and       0.369184  0.325808  0.105479  0.568380  ... -0.207473  0.022182   \n",
       "as       -0.338354 -0.281027 -0.422187 -0.105510  ... -0.264047 -0.070758   \n",
       "\n",
       "                92        93        94        95        96        97   \n",
       "deep      0.215040 -0.053152 -0.465848 -0.186827 -0.131936  0.609785  \\\n",
       "networks  0.248224  0.164831  0.715166 -0.475854 -0.674728 -0.776752   \n",
       "neural    0.477996 -0.104747 -0.834805  0.799360 -0.066161  0.337029   \n",
       "and       0.142096 -0.383148 -0.540944 -0.025271  0.476111 -0.004251   \n",
       "as        0.375570  0.088351 -0.044989  0.018857  0.156525 -0.378772   \n",
       "\n",
       "                98        99  \n",
       "deep     -0.010017  0.745071  \n",
       "networks -0.227652 -0.530007  \n",
       "neural   -0.684721  0.493491  \n",
       "and      -0.621524 -0.243947  \n",
       "as        0.033704  0.002676  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c43c865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 74)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'deep': ['representation', 'with', 'such', 'known', 'recurrent'],\n",
       " 'unsupervised': ['medical', 'science', 'board', 'inspection', 'analysis']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "# view contextually similar words\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['deep', 'unsupervised']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67f367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
